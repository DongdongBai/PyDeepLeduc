''' Trains the neural network.

Uses data generated by @{data_generation_call}.
@module train'''

from Source.Settings.arguments import arguments
from Source.Settings.constants import constants
from Source.Nn.masked_huber_loss import MaskedHuberLoss
import torch.optim as optim
import torch

class M:
    def _save_model(self, model):
        ''' Saves a neural net model to disk.
        
        The model is saved to `arguments.model_path` and labelled with the epoch
        number.
        @param model the neural net to save
        @param epoch the current epoch number
        @param valid_loss the validation loss of the current network
        @local'''
        net_type_str = '_gpu' if arguments.gpu else '_cpu'
        model_file_name = arguments.model_path + arguments.value_net_name + net_type_str + '.pt'
        torch.save(model, model_file_name)

    def train(self, model, data_stream, epoch_count):
        ''' Saves a neural net model to disk.
        
        The model is saved to `arguments.model_path` and labelled with the epoch
        number.
        @param model the neural net to save
        @param epoch the current epoch number
        @param valid_loss the validation loss of the current network
        @local'''
        print(model)
        criterion = MaskedHuberLoss.apply
        optimizer = optim.Adam(model.parameters(), lr=arguments.learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', min_lr=1e-3)

        min_loss = constants.max_number

        # optimization loop
        for epoch in range(epoch_count):
            data_stream.start_epoch()
            model.train()
            lossSum = 0
            for i in range(data_stream.get_train_batch_count()):
                inputs, targets, mask = data_stream.get_train_batch(i)

                optimizer.zero_grad()

                outputs = model(inputs)
                loss = criterion(outputs, targets, mask)

                loss.backward()
                optimizer.step()
                lossSum += loss.item()

            print(f'Training loss: {lossSum / data_stream.train_batch_count}')

            # check validation loss
            model.eval()
            valid_loss_sum = 0
            for i in range(data_stream.get_valid_batch_count()):
                inputs, targets, mask = data_stream.get_valid_batch(i)
                outputs = model(inputs)
                loss = criterion(outputs, targets, mask)
                scheduler.step(loss)
                valid_loss_sum += loss

            valid_loss = valid_loss_sum / data_stream.valid_batch_count
            print(f'Validation loss: {valid_loss}')

            # saving the model
            print(epoch)
            if epoch > 8 and min_loss > valid_loss:
                min_loss = valid_loss
                print("SAVING MODEL")
                self._save_model(model)

train = M()
